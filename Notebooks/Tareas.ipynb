{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a03fb7d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'requests'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrequests\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbs4\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BeautifulSoup\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtime\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'requests'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import csv\n",
    "\n",
    "BASE = \"https://editorial.rottentomatoes.com/guide/best-movies-of-all-time/\"\n",
    "HEADERS = {\"User-Agent\": \"TuProyectoBot/1.0\"}\n",
    "\n",
    "def obtener_urls_lista():\n",
    "    r = requests.get(BASE, headers=HEADERS)\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "    # Ajusta el selector a la estructura real\n",
    "    anchors = soup.select(\"a.article_movie_title\")\n",
    "    return [a['href'] for a in anchors[:300]]\n",
    "\n",
    "def parsear_pelicula(url):\n",
    "    r = requests.get(url, headers=HEADERS)\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "    data = {}\n",
    "    data['title'] = soup.select_one(\"h1\").get_text(strip=True)\n",
    "    data['director'] = soup.select_one(\"li:contains('Director')\").get_text().split(':',1)[1].strip()\n",
    "    data['synopsis'] = soup.select_one(\"div.movie_synopsis\").get_text(strip=True)\n",
    "    # Agregar otros campos seg√∫n el DOM\n",
    "    return data\n",
    "\n",
    "def main():\n",
    "    urls = obtener_urls_lista()\n",
    "    with open(\"movies.csv\", \"w\", newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=['title','director','synopsis'])\n",
    "        writer.writeheader()\n",
    "        for i, url in enumerate(urls,1):\n",
    "            print(f\"[{i}/{len(urls)}] Scraping {url}\")\n",
    "            try:\n",
    "                data = parsear_pelicula(url)\n",
    "                writer.writerow(data)\n",
    "            except Exception as e:\n",
    "                print(\"Error:\", e)\n",
    "            time.sleep(2)  # para respetar el sitio\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
